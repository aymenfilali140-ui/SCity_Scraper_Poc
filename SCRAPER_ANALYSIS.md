# Analysis: How qatar-events-scraper-master Works

## Overview

I've analyzed the existing Python scraper project and discovered **exactly how they successfully scrape these websites**. Here's what I learned:

---

## Key Findings

### Technology Stack
- **Python** (not Node.js)
- **BeautifulSoup** for HTML parsing (similar to Cheerio)
- **Requests** library for HTTP requests (similar to Axios)
- **Pandas** for data management
- **Google Sheets integration** for data storage

### Critical Discovery: They Use **STATIC HTML SCRAPING** Too!

**This is important**: They're using the same approach we are (static HTML parsing), but they have **specific, accurate selectors** that work with the current website structure.

---

## How Each Scraper Works

### 1. ILoveQatar Scraper

**Approach**: Two-step scraping process

#### Step 1: Get Event Links from Listing Pages
```python
# They scrape paginated listing pages
url = "https://www.iloveqatar.net/events/p{page_num}"

# Find all event links using this specific selector:
event_links = soup.find_all("a", class_="article-block__title")
```

**Key Insight**: They found the exact CSS class name (`article-block__title`) that ILoveQatar uses for event titles!

#### Step 2: Scrape Individual Event Pages
For each event link, they visit the full event page and extract:

```python
# Title
title = soup.find("h1").get_text(strip=True)

# Date - using EXACT class name
date_item = soup.find("div", class_="events-page-info__item _date")

# Time - using EXACT class name  
time_item = soup.find("div", class_="events-page-info__item _time")

# Location - using EXACT class name
location_item = soup.find("div", class_="events-page-info__item _location")

# Tickets - using EXACT class name
tickets_items = soup.find_all("div", class_="events-page-info__item _tickets")

# Description
description_div = soup.find("div", {"class": "events-page-info"})
paragraphs = description_div.find_all("p")
```

**Why This Works**:
- They use **precise, website-specific selectors**
- They scrape **individual event pages**, not just the listing
- They handle **pagination** (multiple pages of events)

---

### 2. Visit Qatar Scraper

**This is BRILLIANT!** üéØ

They discovered that Visit Qatar **embeds event data as JSON** in an HTML attribute!

```python
# Find this custom HTML tag
event_listing_tag = soup.find("vq-event-listing")

# Extract the :events attribute which contains JSON data!
raw_events_data = event_listing_tag.get(":events")

# Clean HTML entities and parse as JSON
cleaned_data = self.clean_raw_data(raw_events_data)
event_list = json.loads(f"[{cleaned_data}]")
```

**What's Happening**:
1. Visit Qatar uses a Vue.js component called `<vq-event-listing>`
2. The component has an attribute `:events` that contains **all event data as JSON**
3. Instead of scraping rendered HTML, they extract and parse this JSON directly!

**Example of what they're extracting**:
```html
<vq-event-listing :events="[{title: 'Event 1', startDate: {...}, ...}, ...]">
```

This is **much more reliable** than scraping rendered HTML!

---

### 3. Qatar Museums Scraper

Similar approach to ILoveQatar - they use specific selectors for qm.org.qa.

---

## Why Their Scrapers Work (And Ours Don't)

### Our Approach (Generic Selectors)
```javascript
// We tried generic selectors that might work on any site
$('.event-item, .event-card, article.event, .listing-item')
```

### Their Approach (Website-Specific Selectors)
```python
# They use EXACT selectors for each specific website
soup.find_all("a", class_="article-block__title")  # ILoveQatar specific
soup.find("vq-event-listing")  # Visit Qatar specific
```

**The Difference**:
- ‚ùå **Generic selectors** = might work on many sites, but probably won't work on any
- ‚úÖ **Specific selectors** = only work on one site, but work reliably

---

## How to Implement This in Our Node.js Project

### Option 1: Port Their Exact Approach to JavaScript

Here's how to adapt their ILoveQatar scraper to our Node.js code:

```javascript
class ILoveQatarScraper {
    constructor() {
        this.baseUrl = 'https://www.iloveqatar.net';
        this.listingUrl = 'https://www.iloveqatar.net/events/p';
    }

    async scrape() {
        const allEvents = [];
        
        // Step 1: Get event links from listing page
        for (let page = 1; page <= 3; page++) {
            const listingUrl = `${this.listingUrl}${page}`;
            const response = await axios.get(listingUrl, {
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            });
            
            const $ = cheerio.load(response.data);
            
            // Use their EXACT selector
            const eventLinks = [];
            $('a.article-block__title').each((i, el) => {
                const href = $(el).attr('href');
                if (href) eventLinks.push(href);
            });
            
            // Step 2: Scrape each individual event page
            for (const link of eventLinks) {
                const event = await this.scrapeEventPage(link);
                if (event) allEvents.push(event);
            }
        }
        
        return allEvents.length > 0 ? allEvents : this.getMockEvents();
    }

    async scrapeEventPage(url) {
        try {
            const response = await axios.get(url, {
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            });
            
            const $ = cheerio.load(response.data);
            
            // Use their EXACT selectors
            const title = $('h1').first().text().trim();
            const date = $('.events-page-info__item._date').text().trim().replace('Date:', '').trim();
            const time = $('.events-page-info__item._time').text().trim().replace('Time:', '').trim();
            const location = $('.events-page-info__item._location').text().trim().replace('Location:', '').trim();
            const tickets = $('.events-page-info__item._tickets').first().text().trim().replace('Tickets:', '').trim();
            const prices = $('.events-page-info__item._tickets').eq(1).text().trim().replace('Prices:', '').trim();
            
            // Extract description
            const description = $('.events-page-info p').map((i, el) => $(el).text().trim()).get().join('\\n\\n');
            
            // Extract category from URL
            const urlParts = url.split('/');
            const category = urlParts[5] || 'general';
            
            return {
                id: url,
                title,
                description,
                date,
                time,
                price: prices || 'Check website',
                category,
                image: '', // Would need to extract from page
                link: url,
                venue: location,
                organizer: 'ILoveQatar'
            };
        } catch (error) {
            console.error(`Error scraping ${url}:`, error.message);
            return null;
        }
    }
}
```

### Option 2: Visit Qatar JSON Extraction

This is even better - extract the JSON data directly:

```javascript
class VisitQatarScraper {
    constructor() {
        this.baseUrl = 'https://visitqatar.com/intl-en/events-calendar/all-events';
    }

    async scrape() {
        try {
            const response = await axios.get(this.baseUrl, {
                headers: {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            });
            
            const $ = cheerio.load(response.data);
            
            // Find the vq-event-listing tag
            const eventListingTag = $('vq-event-listing');
            if (!eventListingTag.length) {
                console.log('Could not find vq-event-listing tag');
                return this.getMockEvents();
            }
            
            // Extract the :events attribute
            let rawEventsData = eventListingTag.attr(':events');
            if (!rawEventsData) {
                console.log('No :events attribute found');
                return this.getMockEvents();
            }
            
            // Clean the data
            rawEventsData = rawEventsData
                .replace(/&#34;/g, '"')
                .replace(/&amp;/g, '&')
                .replace(/&nbsp;/g, ' ')
                .replace(/&#39;/g, "'")
                .replace(/\\n/g, '');
            
            // Parse as JSON
            const events = JSON.parse(rawEventsData);
            
            // Transform to our format
            return events.map(event => ({
                id: event.id,
                title: event.title,
                description: event.description?.replace(/<[^>]*>/g, ''),
                date: `${event.startDate?.day} ${event.startDate?.monthAndYear}`,
                time: event.time?.formatted12Hour || '',
                price: event.free ? 'Free' : 'Paid',
                category: event.category?.join(', ') || 'Events',
                image: event.image || '',
                link: event.linkToDetailPage?.url || '',
                venue: event.location?.name || 'Qatar',
                organizer: 'Visit Qatar'
            }));
            
        } catch (error) {
            console.error('Visit Qatar scraping error:', error.message);
            return this.getMockEvents();
        }
    }
}
```

---

## Implementation Recommendations

### Immediate Actions (If You Want Real Data)

1. **Update ILoveQatar Scraper**
   - Use their exact selectors: `a.article-block__title`
   - Implement two-step scraping (listing ‚Üí individual pages)
   - Handle pagination

2. **Update Visit Qatar Scraper**
   - Extract JSON from `vq-event-listing` tag's `:events` attribute
   - Parse and transform the JSON data
   - Much more reliable than HTML scraping!

3. **Test Thoroughly**
   - Websites change their HTML structure frequently
   - Add error handling for when selectors break
   - Always have mock data as fallback

### Long-Term Considerations

**Pros of Implementing Real Scraping**:
- ‚úÖ Real, up-to-date event data
- ‚úÖ Automatic updates
- ‚úÖ More valuable for users

**Cons**:
- ‚ö†Ô∏è Websites can change HTML structure anytime (breaks scrapers)
- ‚ö†Ô∏è Need to maintain and update selectors regularly
- ‚ö†Ô∏è May violate terms of service
- ‚ö†Ô∏è Slower than mock data
- ‚ö†Ô∏è Can be blocked by anti-scraping measures

---

## Code Comparison

### What We Have Now
```javascript
// Generic selectors - don't work
$('.event-item, .event-card, article.event').each((i, element) => {
    const title = $(element).find('h2, h3, .event-title').first().text();
    // ...
});
```

### What They Do (That Works)
```python
# Specific selectors for each site
event_links = soup.find_all("a", class_="article-block__title")  # ILoveQatar
event_listing = soup.find("vq-event-listing")  # Visit Qatar
```

---

## My Recommendation

### For Demo/Development: Keep Mock Data ‚úÖ
Your current approach with mock data is perfect for:
- Demonstrating the UI/UX
- Rapid development
- Reliability
- Legal safety

### For Production: Implement Their Approach ‚ö†Ô∏è
If you need real data:
1. Port their ILoveQatar scraper (two-step process with exact selectors)
2. Port their Visit Qatar scraper (JSON extraction from HTML attribute)
3. Add robust error handling and fallbacks
4. Monitor for when websites change their structure
5. Consider legal implications

### Best Long-Term Solution: Official APIs üéØ
- Contact websites for official API access
- More reliable and legal
- Better data quality
- No maintenance burden

---

## Files to Reference

If you want to implement their approach, study these files:
- [`scrapers/iloveqatar.py`](file:///Users/aymenfilali/.gemini/antigravity/scratch/qatar-events-scraper-master%202/scrapers/iloveqatar.py) - Two-step scraping with exact selectors
- [`scrapers/visitqatar.py`](file:///Users/aymenfilali/.gemini/antigravity/scratch/qatar-events-scraper-master%202/scrapers/visitqatar.py) - JSON extraction from HTML attribute
- [`base_scraper.py`](file:///Users/aymenfilali/.gemini/antigravity/scratch/qatar-events-scraper-master%202/base_scraper.py) - Common scraping utilities

---

## Summary

**The Secret to Their Success**:
1. ‚úÖ **Website-specific selectors** (not generic ones)
2. ‚úÖ **Two-step scraping** for ILoveQatar (listing ‚Üí detail pages)
3. ‚úÖ **JSON extraction** for Visit Qatar (from HTML attributes)
4. ‚úÖ **Pagination handling** (multiple pages of events)
5. ‚úÖ **Robust error handling** with fallbacks

**Key Takeaway**: There's no magic - they just **inspected each website carefully** and found the exact selectors that work. Their scrapers are **website-specific**, not generic.

Would you like me to implement their approach in our Node.js scrapers?
